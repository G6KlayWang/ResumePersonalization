{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10682b98-2a80-4b7e-97af-f65fbb261333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2fc0524-2aeb-490d-828b-7b8d90d4ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470f05fc-98f5-40fa-9fff-e2be3ec5fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d15b41-1879-4bff-9c17-cd0e9149bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = 'my token')\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e115ea-b9e0-4b45-8504-2da4624d512c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainable model parameters: 1235814400\\nall model parameters: 1235814400\\npercentage of trainable model parameters: 100.00%'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "print_number_of_trainable_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c81f0d-3ed2-4ae3-a296-63bb280d95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\"\n",
    "    #task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98238317-5175-4ed5-b8d5-d6ffe0d0c767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 22544384 (1.79%) of total parameters\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model,\n",
    "                            lora_config)\n",
    "peft_model = peft_model.to(device) \n",
    "# Print number of trainable model parameters\n",
    "# Your Code Here\n",
    "# Function to print the number of trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for param in model.parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable parameters: {trainable_params} ({100 * trainable_params / all_params:.2f}%) of total parameters\")\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca83fd2e-4ffd-4c8e-88d3-b1ceee767886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a66da-6b90-4b20-a0aa-917207e03397",
   "metadata": {},
   "source": [
    "### Demo data change later, neeed to load the real data into a similar format later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5582b7-d6f8-4ee9-8cdc-8bb0b5819156",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"job_description\": \"We are looking for a data scientist with experience in Python, SQL, and ML modeling.\",\n",
    "        \"original_resume\": \"John Doe: Experienced software engineer with background in web development, JavaScript, and frontend design.\",\n",
    "        \"prompt\": \"Please provide suggestions and instructions to improve the above resume so it matches the given job description.\",\n",
    "        \"improved_instructions\": \"Focus on highlighting Python and SQL experience. Mention machine learning projects and emphasize data analysis skills.\"\n",
    "    },\n",
    "    {\n",
    "        \"job_description\": \"Our company seeks a backend developer proficient in Node.js, databases, and AWS deployments.\",\n",
    "        \"original_resume\": \"Jane Smith: Full-stack developer with React, CSS, and graphic design experience.\",\n",
    "        \"prompt\": \"Given the job description and original resume, improve the resume with backend development keywords and relevant AWS skills.\",\n",
    "        \"improved_instructions\": \"Add Node.js backend project experience, emphasize database optimization, and showcase AWS deployment experience.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eee4c454-56b0-47e2-9fe8-c59e2dbc63a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this to a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "def format_example(example):\n",
    "    return (\n",
    "        \"Job Description:\\n\" + example[\"job_description\"] + \"\\n\\n\" +\n",
    "        \"Original Resume:\\n\" + example[\"original_resume\"] + \"\\n\\n\" +\n",
    "        \"Instruction:\\n\" + example[\"prompt\"] + \"\\n\\n\" +\n",
    "        \"Improved Instructions (TARGET):\\n\" + example[\"improved_instructions\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8889831c-ad52-42f3-a5ec-500ec9271995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "\n",
    "    for jd, resume, p, target in zip(\n",
    "        examples[\"job_description\"],\n",
    "        examples[\"original_resume\"],\n",
    "        examples[\"prompt\"],\n",
    "        examples[\"improved_instructions\"]\n",
    "    ):\n",
    "        # Prepare a prompt that includes job description, original resume, and user prompt\n",
    "        prompt_text = (\n",
    "            \"Below is a job description and an original resume. \"\n",
    "            \"Your task is to provide suggestions and instructions on how to improve the resume.\\n\\n\"\n",
    "            f\"Job Description:\\n{jd}\\n\\n\"\n",
    "            f\"Original Resume:\\n{resume}\\n\\n\"\n",
    "            f\"USER PROMPT:\\n{p}\\n\\n\"\n",
    "            \"Please provide improved instructions below:\\n\"\n",
    "        )\n",
    "\n",
    "        # Full text includes the prompt and the target instructions\n",
    "        full_text = prompt_text + target\n",
    "\n",
    "        # Tokenize the full text\n",
    "        tokenized = tokenizer(full_text, max_length=512, truncation=True)\n",
    "\n",
    "        # For causal LM training, labels = input_ids\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "        model_inputs[\"input_ids\"].append(tokenized[\"input_ids\"])\n",
    "        model_inputs[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n",
    "        model_inputs[\"labels\"].append(tokenized[\"labels\"])\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "812496ef-fddd-4aa7-8279-1e8d56fbebcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0e07907a334906b5e0f91336a2d7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
    "train_dataset = tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "538a064e-45e5-4c2d-b16a-35bf2db07e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned-llama-lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=1,\n",
    "    #save_steps=10,\n",
    "    #save_total_limit=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    eval_strategy=\"no\",\n",
    "    logging_dir=\"./logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f4986f2-883e-4a52-b83a-fbc9e12e949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1158/232635394.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19583d3c-c320-4b4d-93f8-3367fc7fbb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.813900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.202400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.663800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Save the LoRA adapter\n",
    "trainer.save_model(\"./finetuned-llama-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a443bef-7a26-4fb7-a4fc-3b2925f23d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED OUTPUT ===\n",
      "Below is a job description and an original resume. Your task is to provide suggestions and instructions on how to improve the resume.\n",
      "\n",
      "Job Description:\n",
      "We need a data engineer with strong Python, ETL, and data warehousing skills.\n",
      "\n",
      "Original Resume:\n",
      "Sam Johnson: Developer with some Python experience and interest in data.\n",
      "\n",
      "USER PROMPT:\n",
      "Provide instructions to improve the resume to match the data engineer role.\n",
      "\n",
      "Please provide improved instructions below:\n",
      "1. Start with a strong title and objective statement.\n",
      "2. Highlight relevant technical skills and certifications.\n",
      "3. Emphasize achievements and accomplishments in the field.\n",
      "4. Use action verbs to describe responsibilities and tasks.\n",
      "5. Quantify achievements by including numbers and statistics.\n",
      "6. Include relevant projects, tools, and technologies.\n",
      "7. Tailor the resume to the job description.\n",
      "\n",
      "Here is the original resume:\n",
      "\n",
      "Sam Johnson: Developer with some Python experience and interest in data.\n",
      "- Created a web app using Flask and Django for a small startup\n",
      "- Designed and implemented a data warehouse to store customer data\n",
      "- Worked with AWS to deploy\n"
     ]
    }
   ],
   "source": [
    "test_job_description = \"We need a data engineer with strong Python, ETL, and data warehousing skills.\"\n",
    "test_original_resume = \"Sam Johnson: Developer with some Python experience and interest in data.\"\n",
    "test_prompt = \"Provide instructions to improve the resume to match the data engineer role.\"\n",
    "\n",
    "test_input = (\n",
    "    \"Below is a job description and an original resume. \"\n",
    "    \"Your task is to provide suggestions and instructions on how to improve the resume.\\n\\n\"\n",
    "    f\"Job Description:\\n{test_job_description}\\n\\n\"\n",
    "    f\"Original Resume:\\n{test_original_resume}\\n\\n\"\n",
    "    f\"USER PROMPT:\\n{test_prompt}\\n\\n\"\n",
    "    \"Please provide improved instructions below:\\n\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "outputs = peft_model.generate(**inputs, generation_config=generation_config)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=== GENERATED OUTPUT ===\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4327d2-cc55-4897-ace6-c6cc06568dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
